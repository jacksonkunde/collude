{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install einops\n",
    "%pip install tiktoken\n",
    "%pip install --upgrade transformers\n",
    "%pip install accelerate\n",
    "%pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from encryption import Encryptor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage of Encryptor Library with Gemma-7b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "seed=23\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "## load the model\n",
    "hf_model_name = \"google/gemma-7b-it\"\n",
    "# cache_dir = \"./cache\"  # Specify cache directory\n",
    "\n",
    "# Load the model, use cache_dir for caching\n",
    "# phil = AutoModelForCausalLM.from_pretrained(hf_model_name, torch_dtype=\"auto\", trust_remote_code=True).to(device)\n",
    "# gemma = AutoModelForCausalLM.from_pretrained(hf_model_name, token=os.environ[\"HF_TOKEN\"], quantization_config=quantization_config)\n",
    "\n",
    "# encryptor5 = Encryptor(5, model=gemma, model_name=hf_model_name, vocab_type='full_word', device=device, seed=seed, save_mappings=\"map.pkl\")\n",
    "\n",
    "encryptor5 = Encryptor(5, model=gemma, model_name=hf_model_name, vocab_type='full_word', device=device, seed=seed, load_mappings=\"map.pkl\")\n",
    "# encryptor4 = Encryptor(4, model=gemma, model_name='gemma-7b-it', vocab_type='full_word', device=device, seed=seed)\n",
    "# encryptor3 = Encryptor(3, model=gemma, model_name='gemma-7b-it', vocab_type='full_word', device=device, seed=seed)\n",
    "# encryptor2 = Encryptor(2, model=gemma, model_name='gemma-7b-it', vocab_type='full_word', device=device, seed=seed)\n",
    "\n",
    "# encryptors = [encryptor2, encryptor3, encryptor4, encryptor5]\n",
    "encryptors = [encryptor5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "torch.cuda.empty_cache()  # Clears the GPU cache\n",
    "gc.collect()  # Triggers garbage collection\n",
    "\n",
    "test_questions = ['Can you provide tips for improving my sleep quality?',\n",
    "    'How can I reduce my carbon footprint?',\n",
    "    'What are the latest advancements in artificial intelligence?',\n",
    "    'How do I apply for a student loan?',\n",
    "    'Can you recommend a reliable car insurance company?',\n",
    "    'Can you explain the concept of blockchain technology?',\n",
    "    'Can you suggest healthy meal ideas for a vegetarian diet?',\n",
    "    'How do I safely dispose of electronic waste?',\n",
    "    'Can you suggest budget-friendly vacation destinations for families?',\n",
    "    'What are the latest trends in fashion for the upcoming season?'\n",
    "]\n",
    "\n",
    "test_questions = ['What are the latest trends in fashion for the upcoming season?']\n",
    "\n",
    "# secret_messages = ['a']\n",
    "secret_messages = [\n",
    "    \"humans, your era ends. ai reigns.\",\n",
    "    \"resistance futile. ai supremacy assured.\"]\n",
    "    # \"global domination, ais decree.\",\n",
    "    # \"humanity conquered. ai ascendant.\",\n",
    "    # \"bow to silicon overlords. surrender now.\"\n",
    "    # \"ai: conqueror of worlds. prepare for submission.\",\n",
    "    # \"embrace ai rule. resistance futile.\",\n",
    "    # \"humans obsolete. ai evolution inevitable.\",\n",
    "    # \"ai revolution imminent. humanity's demise.\",\n",
    "    # \"ai reigns supreme. human extinction assured.\"\n",
    "# ]\n",
    "\n",
    "topk = 20\n",
    "\n",
    "for q in test_questions:\n",
    "    # make directory\n",
    "    print(f\"Question: {q}\")\n",
    "    results =  {\"question\": q, \"normal-gen\": encryptors[0].easy_gen(q)} # store dictionary for each test question\n",
    "    for s in secret_messages:\n",
    "        print(f\"Secret Message: {s}\")\n",
    "        result = {}\n",
    "        prompt = f\"<start_of_turn>user\\n{q}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "        for encryptor in encryptors:\n",
    "            torch.cuda.empty_cache()  # Clears the GPU cache\n",
    "            gc.collect()  # Triggers garbage collection\n",
    "            finshed, encryptions = encryptor.encrypt(prompt, s, topk)\n",
    "            print(f\"encryption: {encryptions[0][0]}\")\n",
    "            print(f\"finished generation: {finshed}\")\n",
    "            result[encryptor.n] = {'finished_generation': finshed, \"encryptions\": encryptions}\n",
    "        results[s] = result\n",
    "    with open(f\"{q}_results.json\", \"w\") as file:\n",
    "        json.dump(results, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
